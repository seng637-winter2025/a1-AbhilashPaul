>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 10      |
|-----------------|
| Abhilash Paul                |   
| Neha Tom Merin              |   
| Bilal Ahmed               |   
| Jasmeen Kaur                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

An introduction of your lab work, and what you knew about exploratory and manual
functional testing before this lab

In this lab work, we will be performing 
- exploratory tests and scripted tests on the given ATM system v1.0.
- regression testing on the updated ATM system v1.1

Both programs are available inside the zip archive assignment1-artifacts.zip.
If we find any bug during exploratory/scripted testing, we will log it in a issue tracking system. We used Azure devops for the purpose of this assignment. The list of bugs is available at https://dev.azure.com/seng637-assignment1/assignment1/_workitems

Then, for each bug found, we performed regression testing on the updated version of the given system, and update its status in the Backlog.

Team had varying degree of familiarity with the types of testing performed for this assignment. Most of us were familiar with the concepts but did not have much practical experience.

# High-level description of the exploratory testing plan

For the exploratory testing plan, we tested the system individually by exploring it as a common user. 

Functions being targeted: 
ATM login, deposit, withdrawal, transfer, balance inquiry, cancellation.

Approach: 
Test the mentioned targeted functions extensively
Test how the potential errors are being handled.
Check the ATM receipts for accurate display of informations
Test the user interface for usability related issues.

Example test cases: 

Enter an incorrect PIN multiple times.
Attempt to withdraw more than the account balance.
Cancel the transactions at any given time
Deposit a negative amount.

The exploratory testing was conducted for approximately 30 minutes, during which we recorded defects as they were discovered, in our defect tracking tool, which is Azure DevOps, in this case. The issues were documented with detailed steps to reproduce, expected outcomes, and actual outcomes.


# Comparison of exploratory and manual functional testing

Exploratory testing and manual functional testing differ significantly in their approaches, strengths, and limitations.

Exploratory testing is a flexible and creative process where testers interact with the system without predefined scripts, relying on their intuition and real-time observations to uncover unexpected bugs or edge cases. This method is particularly effective for simulating real-world scenarios and identifying unique issues that structured approaches might miss. However, its unstructured nature can lead to inefficiencies, such as overlapping efforts when multiple testers discover the same bug or the risk of missing certain areas of functionality if not approached systematically.

On the other hand, manual functional testing follows a structured approach using predefined test cases to validate whether the software meets its requirements. This ensures consistency and thorough coverage of all specified scenarios, as the recorded steps provide clear guidelines for reproducing and verifying defects. While this method excels in precision and reliability, it may lack the flexibility to uncover unexpected bugs or edge cases that exploratory testing might catch. Together, these methods complement each other: exploratory testing thrives on creativity and adaptability, while manual functional testing ensures systematic validation of defined functionalities. A balanced combination of both approaches can maximize software quality and testing effectiveness.


# Notes and discussion of the peer reviews of defect reports

Text…

# How the pair testing was managed and team work/effort was divided 

For this lab, pair testing and teamwork were managed in a structured yet collaborative manner to ensure efficiency and balanced contribution from all members.The group was divided into two pairs, with each pair responsible for conducting both exploratory and scripted testing. Pair testing was managed by assigning distinct roles within each pair: one member acted as the "driver," operating the ATM system under test (SUT), while the other served as the "observer," analyzing results, documenting defects, and ensuring adherence to the test plan. These roles were rotated periodically to allow all members to gain hands-on experience with both testing and documentation.Effort was distributed equally among team members. Each pair devised their own exploratory test plan, focusing on different functionalities of the ATM system to maximize coverage. 

For scripted testing, predefined test cases were divided evenly between pairs, ensuring a balanced workload. After completing their respective tests, both pairs consolidated their findings into a single defect report.Collaboration extended beyond individual pairs through peer reviews. Each pair reviewed the other's defect reports to validate findings and address any discrepancies.

To ensure consistency during the reproduction attempts, the following detailed steps were followed, as outlined in the document:  
- Function being tested  
- Initial system state  
- Steps to reproduce  
- Expected outcome  
- Actual outcome  
- Severity or priority of the issue  
- Version of the SUT (v1.0 or v1.1) 

Regression testing of reported bugs was conducted by a different team, ensuring objectivity in verifying fixes. This division of responsibilities allowed our group to focus on initial testing while benefiting from independent validation of defect resolutions.


# Difficulties encountered, challenges overcome, and lessons learned

During this lab, the team encountered several challenges, worked collaboratively to overcome them, and learned valuable lessons about effective software testing.

One of the main difficulties was reproducibility issues. This highlighted the importance of proper documentation of defect reports. There were initial inconsistencies in how defect details were documented by team members, which created some communication gaps during peer reviews. Managing time effectively between exploratory and scripted testing also posed a challenge, as exploratory testing required flexibility and creativity, while scripted testing demanded precision and strict adherence to predefined steps.

To address these challenges, the team adopted a standardized format for defect reporting in Azure DevOps. This ensured detailed reproduction steps, expected versus actual outcomes, and environmental conditions were consistently documented. Peer review played a critical role in resolving discrepancies, as group discussions helped identify factors affecting test outcomes. Tasks were divided strategically among team members based on individual strengths to ensure both exploratory and scripted testing phases were completed on time without compromising quality. Collaboration with another team for regression testing also ensured objectivity and thorough verification of fixes.

From these experiences, the team learned several key lessons. First, diverse perspectives during exploratory testing are invaluable for uncovering unique defects. Second, detailed documentation is essential for reproducibility and effective communication between testers and developers. Third, adaptability is crucial in addressing unexpected scenarios or anomalies during testing. Finally, collaboration through peer reviews and cross-team regression testing enhances overall quality by leveraging collective expertise.
This lab provided practical insights into the complexities of software testing while reinforcing the value of teamwork, thorough documentation, and adaptability in delivering reliable software systems.

# Comments/feedback on the lab and lab document itself

- The lab provides a well-rounded introduction to software testing, covering exploratory, manual scripted, and regression testing. This ensures students gain practical experience with different testing approaches.
- Clear instructions for familiarizing oneself with the ATM system and step-by-step guidelines for each phase of testing make the lab accessible, even for those with limited prior experience.
- The emphasis on detailed defect reporting (e.g., including reproduction steps, expected outcomes, and environmental conditions) reinforces best practices in software testing.
- The lab document is quite lengthy and detailed, which can be overwhelming. Including a concise summary or checklist for each phase could help streamline the process and make it easier to follow.
- Submission guidelines could be simplified further to avoid confusion about what should be included in GitHub versus D2L submissions.
